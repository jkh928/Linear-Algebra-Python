# Linear Algebra & NumPy: AI Foundations

> **Status:** Active Study 
> **Focus:** Mathematical Foundations for Deep Learning

## ðŸ“– Project Overview
This repository documents my progression through the mathematical concepts essential for Artificial Intelligence and Machine Learning. The goal is to move beyond high-level libraries and understand the low-level linear algebra operations that drive modern neural networks.

I am utilizing **Python** and **NumPy** to implement these concepts manually, bridging the gap between abstract mathematical theory and executable code.

## ðŸ›  Tech Stack & Tools
* **Language:** Python 3.x
* **Core Library:** NumPy (for vectorization and matrix manipulation)
* **Visualization:** Matplotlib / Seaborn
* **Environment:** Jupyter Notebooks (Anaconda)

## ðŸ“‚ Repository Structure

### `01-Course-Notes/`
**Primary Resource:** *Master Linear Algebra: Theory and Implementation in Code* (Mike X Cohen)
* Detailed code-along notebooks.
* Implementation of core concepts:
    * Vectors & Spaces (Dot products, spans, basis vectors)
    * Matrix Operations (Multiplication, Inverse, Rank)
    * Eigendecomposition & SVD
    * Least Squares & Projections

### `02-Experiments/`
* My own sandbox for testing concepts.
* Independent challenges and implementations of algorithms from scratch without relying on pre-built solver functions.

## ðŸš€ Why This Matters for AI
Understanding these concepts is critical for:
* **Dimensionality Reduction** (PCA, SVD)
* **Data Transformation** (Basis changes, Linear mappings)
* **Optimization** (Gradient descent landscapes)

---
*Author: Josh Hasam*